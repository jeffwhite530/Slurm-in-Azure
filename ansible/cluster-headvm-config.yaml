---
- name: Cluster configuration, head node
  hosts: all
  become: true
  vars:
     cluster_parameters: "{{ lookup('file', '/var/lib/waagent/custom-script/download/0/clusterParameters.json') | from_json }}"

  tasks:
  - name: Get VM instance metadata
    ansible.builtin.uri:
      url: http://169.254.169.254/metadata/instance?api-version=2021-02-01
      headers:
        Metadata: true
    register: vm_instance_metadata
    changed_when: false

  - name: Get cluster resource group guid string
    ansible.builtin.set_fact:
      resource_group_guid_string: "{{ vm_instance_metadata.json.compute['name'].split('-')[-1] }}"
    changed_when: false

  # TODO: Copy munge key from head to compute nodes
  - name: Verify munge key exists
    ansible.builtin.stat:
      name: /etc/munge/munge.key
    register: munge_key

  - name: Start munge
    ansible.builtin.service:
      name: munge
      state: started
      enabled: true
    when: munge_key.stat.exists

  - name: Start mariadb
    ansible.builtin.service:
      name: mariadb
      state: started
      enabled: true

  - name: Create Slurm user in MariaDB
    community.mysql.mysql_user:
      name: slurm_db_user
      # FIXME: Set a better password, custom for each cluster install
      password: 12345
      priv: '*.*:ALL'
      state: present
      login_unix_socket: /var/run/mysqld/mysqld.sock

  - name: Create slurmctld spool directory
    ansible.builtin.file:
      dest: "{{ item }}"
      owner: slurm
      group: slurm
      mode: 0700
      state: directory
    loop:
      - /var/spool/slurmctld
      - /var/spool/slurmctld/resv_state
      - /var/spool/slurmctld/trigger_state

  - name: Create slurm.conf
    ansible.builtin.copy:
      dest: /etc/slurm/slurm.conf
      owner: root
      group: root
      mode: 0644
      content: |
        ClusterName={{ cluster_parameters['clusterName']['value'] }}
        SlurmctldHost={{ vm_instance_metadata.json.compute.name }}
        #DisableRootJobs=NO
        #EnforcePartLimits=NO
        #Epilog=
        #EpilogSlurmctld=
        #FirstJobId=1
        #MaxJobId=67043328
        #GresTypes=
        #GroupUpdateForce=0
        #GroupUpdateTime=600
        #JobFileAppend=0
        #JobRequeue=1
        #JobSubmitPlugins=lua
        #KillOnBadExit=0
        #LaunchType=launch/slurm
        #Licenses=foo*4,bar
        #MailProg=/bin/mail
        #MaxJobCount=10000
        #MaxStepCount=40000
        #MaxTasksPerNode=512
        MpiDefault=none
        #MpiParams=ports=#-#
        #PluginDir=
        #PlugStackConfig=
        #PrivateData=jobs
        ProctrackType=proctrack/cgroup
        #Prolog=
        #PrologFlags=
        #PrologSlurmctld=
        #PropagatePrioProcess=0
        #PropagateResourceLimits=
        #PropagateResourceLimitsExcept=
        #RebootProgram=
        ReturnToService=2
        SlurmctldPidFile=/var/run/slurmctld.pid
        SlurmctldPort=6817
        SlurmdPidFile=/var/run/slurmd.pid
        SlurmdPort=6818
        SlurmdSpoolDir=/var/spool/slurmd
        SlurmUser=slurm
        #SlurmdUser=root
        #SrunEpilog=
        #SrunProlog=
        StateSaveLocation=/var/spool/slurmctld
        SwitchType=switch/none
        #TaskEpilog=
        #TaskPlugin=task/cgroup
        #TaskProlog=
        #TopologyPlugin=topology/tree
        #TmpFS=/tmp
        #TrackWCKey=no
        #TreeWidth=
        #UnkillableStepProgram=
        #UsePAM=0
        #
        #
        # TIMERS
        #BatchStartTimeout=10
        #CompleteWait=0
        #EpilogMsgTime=2000
        #GetEnvTimeout=2
        #HealthCheckInterval=0
        #HealthCheckProgram=
        InactiveLimit=0
        KillWait=30
        #MessageTimeout=10
        #ResvOverRun=0
        MinJobAge=300
        #OverTimeLimit=0
        SlurmctldTimeout=120
        SlurmdTimeout=300
        #UnkillableStepTimeout=60
        #VSizeFactor=0
        Waittime=0
        #
        #
        # SCHEDULING
        #DefMemPerCPU=0
        #MaxMemPerCPU=0
        #SchedulerTimeSlice=30
        SchedulerType=sched/backfill
        SelectType=select/cons_tres
        SelectTypeParameters=CR_Core_Memory
        #
        #
        # JOB PRIORITY
        #PriorityFlags=
        #PriorityType=priority/basic
        #PriorityDecayHalfLife=
        #PriorityCalcPeriod=
        #PriorityFavorSmall=
        #PriorityMaxAge=
        #PriorityUsageResetPeriod=
        #PriorityWeightAge=
        #PriorityWeightFairshare=
        #PriorityWeightJobSize=
        #PriorityWeightPartition=
        #PriorityWeightQOS=
        #
        #
        # LOGGING AND ACCOUNTING
        #AccountingStorageEnforce=0
        #AccountingStorageHost=
        #AccountingStoragePass=
        #AccountingStoragePort=
        AccountingStorageType=accounting_storage/slurmdbd
        #AccountingStorageUser=
        #AccountingStoreFlags=
        #JobCompHost=
        #JobCompLoc=
        #JobCompPass=
        #JobCompPort=
        JobCompType=jobcomp/none
        #JobCompUser=
        #JobContainerType=job_container/none
        JobAcctGatherFrequency=30
        JobAcctGatherType=jobacct_gather/cgroup
        SlurmctldDebug=info
        SlurmctldLogFile=/var/log/slurmctld.log
        SlurmdDebug=info
        SlurmdLogFile=/var/log/slurmd.log
        #SlurmSchedLogFile=
        #SlurmSchedLogLevel=
        #DebugFlags=
        #
        #
        # POWER SAVE SUPPORT FOR IDLE NODES (optional)
        #SuspendProgram=
        #ResumeProgram=
        #SuspendTimeout=
        #ResumeTimeout=
        #ResumeRate=
        #SuspendExcNodes=
        #SuspendExcParts=
        #SuspendRate=
        #SuspendTime=
        #
        #
        # COMPUTE NODES
        PartitionName=main Nodes=ALL Default=YES MaxTime=INFINITE State=UP

  - name: Add compute nodes to slurm.conf
    ansible.builtin.lineinfile:
      path: /etc/slurm/slurm.conf
      line: "NodeName={{ cluster_parameters['clusterName']['value'] }}-computevm{{ item }}-{{ resource_group_guid_string }} State=UNKNOWN"
    loop: "{{ range(0, cluster_parameters['computeVMCount']['value'])|list }}"

  - name: Create slurmdbd.conf
    ansible.builtin.copy:
      dest: /etc/slurm/slurmdbd.conf
      owner: slurm
      group: slurm
      mode: 0600
      content: |
        ArchiveEvents=yes
        ArchiveJobs=yes
        ArchiveResvs=yes
        ArchiveSteps=no
        ArchiveSuspend=no
        ArchiveTXN=no
        ArchiveUsage=no
        #ArchiveScript=/usr/sbin/slurm.dbd.archive
        AuthInfo=/var/run/munge/munge.socket.2
        AuthType=auth/munge
        DbdHost={{ vm_instance_metadata.json.compute.name }}
        DebugLevel=info
        PurgeEventAfter=1month
        PurgeJobAfter=3month
        PurgeResvAfter=1month
        PurgeStepAfter=1month
        PurgeSuspendAfter=1month
        PurgeTXNAfter=3month
        PurgeUsageAfter=6month
        LogFile=/var/log/slurmdbd.log
        PidFile=/var/run/slurmdbd.pid
        SlurmUser=slurm
        StoragePass=12345
        StorageType=accounting_storage/mysql
        StorageUser=slurm_db_user

  - name: Start slurmdbd
    ansible.builtin.service:
      name: slurmdbd
      state: started
      enabled: true

  - name: Start slurmctld
    ansible.builtin.service:
      name: slurmctld
      state: started
      enabled: true

  - name: Create the compute node inventory file
    ansible.builtin.copy:
      dest: /var/lib/waagent/custom-script/download/0/ansible/compute_hosts
      owner: root
      group: root
      mode: 0644
      content: |
        [compute_hosts]

  - name: Add the compute node inventory file
    ansible.builtin.lineinfile:
      path: /var/lib/waagent/custom-script/download/0/ansible/compute_hosts
      line: "{{ cluster_parameters['clusterName']['value'] }}-computevm{{ item }}-{{ resource_group_guid_string }}"
    loop: "{{ range(0, cluster_parameters['computeVMCount']['value'])|list }}"